{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning_with_pytorch",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUYgX83CnKdL"
      },
      "source": [
        "## reference https://www.youtube.com/watch?v=c36lUUr864M&t=4380s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibUV6Lxllun6",
        "outputId": "6c09c732-8a70-458c-a2da-3ce5c95cc7f4"
      },
      "source": [
        "import torch\n",
        "import inspect\n",
        "\n",
        "def retrieve_name(var):\n",
        "    callers_local_vars = globals().items()\n",
        "    return [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "\n",
        "def printf(x):\n",
        "    print(f'{retrieve_name(x)}: {x}')\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "printf(x)\n",
        "\n",
        "y = x + 2\n",
        "printf(y)\n",
        "\n",
        "z = y * y * 2\n",
        "z = z.mean()\n",
        "printf(z)\n",
        "\n",
        "z.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['x']: tensor([-0.1026,  0.1340,  1.3998], requires_grad=True)\n",
            "['y']: tensor([1.8974, 2.1340, 3.3998], grad_fn=<AddBackward0>)\n",
            "['z']: 13.141792297363281\n",
            "tensor([2.5299, 2.8453, 4.5330])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb1_fn5gnDoV",
        "outputId": "ece88b8d-8ff2-4e76-c09f-4bd03d561eb9"
      },
      "source": [
        "3.5529 * 3.5529 * 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25.24619682"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMstsjORDP3V"
      },
      "source": [
        "## implementing using numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npFY9fKS9Gpk",
        "outputId": "8f6b750f-19c1-4a6b-8724-c925ea7eb900"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "X = np.array([1,2,3,4], dtype=np.float32)\n",
        "y = np.array([2,4,6,8], dtype=np.float32)\n",
        "\n",
        "w = 0\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted - y)**2).mean()\n",
        "\n",
        "\n",
        "# gradient \n",
        "# mse = 1/N * (w*x - y)**2\n",
        "# dJ/dw = 1/N 2x (w*x - y)\n",
        "def gradient(x, y, y_predicted):\n",
        "    return np.dot(2*x, y_predicted - y).mean()\n",
        "\n",
        "print(f'prediction before training: f(5) = {forward(5)}')\n",
        "\n",
        "# training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    #forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    #loss\n",
        "    l = loss(y, y_pred)\n",
        "\n",
        "    # gradients\n",
        "    dw = gradient(X, y, y_pred)\n",
        "\n",
        "    # updated the weights\n",
        "    w -= learning_rate * dw\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        print(f'epoch {epoch+1}: w = {w}, loss = {l}')\n",
        "\n",
        "print(f'prediction after training: f(5) = {forward(5)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction before training: f(5) = 0\n",
            "epoch 1: w = 1.2, loss = 30.0\n",
            "epoch 2: w = 1.6799999618530272, loss = 4.799999237060547\n",
            "epoch 3: w = 1.871999988555908, loss = 0.7680001854896545\n",
            "epoch 4: w = 1.9487999868392942, loss = 0.1228799968957901\n",
            "epoch 5: w = 1.9795200133323667, loss = 0.019660834223031998\n",
            "epoch 6: w = 1.9918080282211301, loss = 0.0031457357108592987\n",
            "epoch 7: w = 1.9967231869697568, loss = 0.0005033080233260989\n",
            "epoch 8: w = 1.99868928194046, loss = 8.053186320466921e-05\n",
            "epoch 9: w = 1.999475698471069, loss = 1.2884394891443662e-05\n",
            "epoch 10: w = 1.999790253639221, loss = 2.0613531432900345e-06\n",
            "epoch 11: w = 1.9999160599708554, loss = 3.297340072094812e-07\n",
            "epoch 12: w = 1.9999664139747617, loss = 5.282345227897167e-08\n",
            "epoch 13: w = 1.9999865984916685, loss = 8.487816671731707e-09\n",
            "epoch 14: w = 1.9999946093559262, loss = 1.3369572116062045e-09\n",
            "epoch 15: w = 1.9999978351593015, loss = 2.1679014139408537e-10\n",
            "epoch 16: w = 1.9999991369247434, loss = 3.531397396727698e-11\n",
            "epoch 17: w = 1.9999996304512022, loss = 5.076827847005916e-12\n",
            "epoch 18: w = 1.999999837875366, loss = 8.988365607365267e-13\n",
            "epoch 19: w = 1.9999999165534972, loss = 1.3145040611561853e-13\n",
            "epoch 20: w = 1.9999999952316283, loss = 1.3145040611561853e-13\n",
            "prediction before training: f(5) = 9.999999976158142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljev6ipRFfYV"
      },
      "source": [
        "## linear regression using torch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86pvqY8NC-ut",
        "outputId": "64fb8d30-0a69-4882-cbba-1fccc064b53b"
      },
      "source": [
        "import torch\n",
        "\n",
        "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
        "y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted - y)**2).mean()\n",
        "\n",
        "\n",
        "# gradient \n",
        "# mse = 1/N * (w*x - y)**2\n",
        "# dJ/dw = 1/N 2x (w*x - y)\n",
        "def gradient(x, y, y_predicted):\n",
        "    return np.dot(2*x, y_predicted - y).mean()\n",
        "\n",
        "print(f'prediction before training: f(5) = {forward(5)}')\n",
        "\n",
        "# training\n",
        "learning_rate = 0.01\n",
        "n_iters = 50\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    #forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    #loss\n",
        "    l = loss(y, y_pred)\n",
        "\n",
        "    # gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # updated the weights\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "\n",
        "    # zero the gradients\n",
        "    w.grad.zero_()\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == n_iters-1:\n",
        "        print(f'epoch {epoch+1}: w = {w}, loss = {l}')\n",
        "\n",
        "print(f'prediction after training: f(5) = {forward(5)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction before training: f(5) = 0.0\n",
            "epoch 1: w = 0.29999998211860657, loss = 30.0\n",
            "epoch 6: w = 1.2457009553909302, loss = 5.9062323570251465\n",
            "epoch 11: w = 1.6653136014938354, loss = 1.1627856492996216\n",
            "epoch 16: w = 1.8514978885650635, loss = 0.22892260551452637\n",
            "epoch 21: w = 1.934108853340149, loss = 0.0450688973069191\n",
            "epoch 26: w = 1.9707638025283813, loss = 0.008872910402715206\n",
            "epoch 31: w = 1.987027645111084, loss = 0.0017468547448515892\n",
            "epoch 36: w = 1.99424409866333, loss = 0.00034391897497698665\n",
            "epoch 41: w = 1.9974461793899536, loss = 6.770494655938819e-05\n",
            "epoch 46: w = 1.9988667964935303, loss = 1.3328777640708722e-05\n",
            "epoch 50: w = 1.999408483505249, loss = 3.632284688137588e-06\n",
            "prediction after training: f(5) = 9.997042655944824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q-Jbq3mFbjq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0PltLnsitvx"
      },
      "source": [
        "## automatic torch training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dMVrALLiveQ",
        "outputId": "b6b226f5-8e83-4ec9-bdde-409b05c25eae"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
        "y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
        "X_test = torch.tensor([[5]], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "\n",
        "print(f'prediction before training: f(5) = {model(X_test).item()}')\n",
        "\n",
        "# training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    #forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    #loss\n",
        "    l = loss(y, y_pred)\n",
        "\n",
        "    # gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # updated the weights\n",
        "    optimiser.step()\n",
        "\n",
        "    # zero the gradients\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    if epoch % 20 == 0 or epoch == n_iters-1:\n",
        "        w, b = model.parameters()\n",
        "        print(f'epoch {epoch+1}: w = {w[0][0].item()}, loss = {l}')\n",
        "\n",
        "print(f'prediction before training: f(5) = {model(X_test).item()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "prediction before training: f(5) = -1.0928696393966675\n",
            "epoch 1: w = -0.19662858545780182, loss = 36.66752243041992\n",
            "epoch 21: w = 0.3620397746562958, loss = 18.73700714111328\n",
            "epoch 41: w = 0.7613598108291626, loss = 9.594871520996094\n",
            "epoch 61: w = 1.0468980073928833, loss = 4.933378219604492\n",
            "epoch 81: w = 1.2511903047561646, loss = 2.556285858154297\n",
            "epoch 101: w = 1.3974684476852417, loss = 1.3438680171966553\n",
            "epoch 121: w = 1.502320408821106, loss = 0.7252501249313354\n",
            "epoch 141: w = 1.5775905847549438, loss = 0.4093782305717468\n",
            "epoch 161: w = 1.6317358016967773, loss = 0.24786363542079926\n",
            "epoch 181: w = 1.6707953214645386, loss = 0.16505113244056702\n",
            "epoch 201: w = 1.699080467224121, loss = 0.12236984074115753\n",
            "epoch 221: w = 1.719670057296753, loss = 0.10015459358692169\n",
            "epoch 241: w = 1.73476243019104, loss = 0.08837875723838806\n",
            "epoch 261: w = 1.7459272146224976, loss = 0.08193078637123108\n",
            "epoch 281: w = 1.7542853355407715, loss = 0.07820448279380798\n",
            "epoch 301: w = 1.7606370449066162, loss = 0.07587084174156189\n",
            "epoch 321: w = 1.7655545473098755, loss = 0.07425245642662048\n",
            "epoch 341: w = 1.7694449424743652, loss = 0.07300378382205963\n",
            "epoch 361: w = 1.772599697113037, loss = 0.07194867730140686\n",
            "epoch 381: w = 1.7752273082733154, loss = 0.07099734991788864\n",
            "epoch 401: w = 1.7774757146835327, loss = 0.07010383158922195\n",
            "epoch 421: w = 1.7794517278671265, loss = 0.06924453377723694\n",
            "epoch 441: w = 1.7812310457229614, loss = 0.0684075802564621\n",
            "epoch 461: w = 1.782867193222046, loss = 0.06758666783571243\n",
            "epoch 481: w = 1.784399390220642, loss = 0.06677868217229843\n",
            "epoch 500: w = 1.7857835292816162, loss = 0.06602159887552261\n",
            "prediction before training: f(5) = 9.5573091506958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUJivm7OltfN",
        "outputId": "4fe97d55-d380-43e9-b2e3-d84ea5f4b072"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
        "y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
        "X_test = torch.tensor([[5]], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "\n",
        "print(f'prediction before training: f(5) = {model(X_test).item()}')\n",
        "\n",
        "# training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    #forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    #loss\n",
        "    l = loss(y, y_pred)\n",
        "\n",
        "    # gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # updated the weights\n",
        "    optimiser.step()\n",
        "\n",
        "    # zero the gradients\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    if epoch % 20 == 0 or epoch == n_iters-1:\n",
        "        w, b = model.parameters()\n",
        "        print(f'epoch {epoch+1}: w = {w[0][0].item()}, loss = {l}')\n",
        "\n",
        "print(f'prediction before training: f(5) = {model(X_test).item()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "prediction before training: f(5) = 1.507454752922058\n",
            "epoch 1: w = 0.6909193396568298, loss = 23.828542709350586\n",
            "epoch 21: w = 1.9872854948043823, loss = 0.0166370440274477\n",
            "epoch 41: w = 2.019594192504883, loss = 0.000620208156760782\n",
            "epoch 61: w = 2.019270658493042, loss = 0.0005406474811024964\n",
            "epoch 81: w = 2.01816987991333, loss = 0.0004795403510797769\n",
            "epoch 100: w = 2.017164468765259, loss = 0.00042789484723471105\n",
            "prediction before training: f(5) = 10.035356521606445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dkeYDtAnR4Z"
      },
      "source": [
        "## chapter 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "MQ9mZGVCnOf8",
        "outputId": "e20183a6-8984-486a-ca54-ad58b73a2a0d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# data\n",
        "X_np, y_np = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
        "\n",
        "X = torch.from_numpy(X_np.astype(np.float32))\n",
        "y = torch.from_numpy(y_np.astype(np.float32))\n",
        "y = y.view(y.shape[0], 1)\n",
        "\n",
        "# model\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# loss and optimiser\n",
        "learning_rate = 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # forward pass\n",
        "    y_predicted = model(X)\n",
        "    loss = criterion(y_predicted, y)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    optimiser.step()\n",
        "\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    if epoch > 0 and (epoch % 10 == 0 or epoch+1==num_epochs):\n",
        "        print(f'epoch = {epoch}, loss = {loss.item()}')\n",
        "\n",
        "# plot\n",
        "predicted = model(X).detach().numpy()\n",
        "plt.plot(X_np, y_np, 'ro')\n",
        "plt.plot(X_np, predicted, 'b')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 1\n",
            "epoch - 10, loss = 4235.01123046875\n",
            "epoch - 20, loss = 3165.249267578125\n",
            "epoch - 30, loss = 2390.43505859375\n",
            "epoch - 40, loss = 1828.6893310546875\n",
            "epoch - 50, loss = 1421.046142578125\n",
            "epoch - 60, loss = 1124.9803466796875\n",
            "epoch - 70, loss = 909.7840576171875\n",
            "epoch - 80, loss = 753.25537109375\n",
            "epoch - 90, loss = 639.325439453125\n",
            "epoch - 99, loss = 563.516357421875\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBUj67i14cJ6"
      },
      "source": [
        "## logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3Qz1tzl3lO2",
        "outputId": "4521741f-f1ab-4952-fda3-a24fed502abc"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# prepare the data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# scale\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "y_test = y_test.view(y_test.shape[0], 1)\n",
        "\n",
        "# mode;\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, n_input_features):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(n_input_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.lin(x)\n",
        "        y_predicted = torch.sigmoid(logits)\n",
        "        return y_predicted\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "\n",
        "# loss and optimiser\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # forward pass\n",
        "    y_predicted = model(X_train)\n",
        "    loss = criterion(y_predicted, y_train)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    optimiser.step()\n",
        "\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    if epoch > 0 and (epoch % 10 == 0 or epoch+1==num_epochs):\n",
        "        print(f'epoch = {epoch}, loss = {loss.item()}')\n",
        "\n",
        "\n",
        "# benchmark\n",
        "with torch.no_grad():\n",
        "    y_predicted = model(X_test)\n",
        "    y_predicted_cls = y_predicted.round()\n",
        "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "    print(f'accuracy = {acc}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 10, loss = 0.6066616773605347\n",
            "epoch = 20, loss = 0.48761555552482605\n",
            "epoch = 30, loss = 0.4159747064113617\n",
            "epoch = 40, loss = 0.3682396709918976\n",
            "epoch = 50, loss = 0.333825945854187\n",
            "epoch = 60, loss = 0.30757975578308105\n",
            "epoch = 70, loss = 0.2867344319820404\n",
            "epoch = 80, loss = 0.269671231508255\n",
            "epoch = 90, loss = 0.2553757429122925\n",
            "epoch = 99, loss = 0.24431678652763367\n",
            "accuracy = 0.9035087823867798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cHqomm369zr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usQXPUQE6Gsj"
      },
      "source": [
        "## loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxBJT_M7-GNI",
        "outputId": "8648999f-5840-49c5-c1e5-ed7ff10bb21c"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
        "!head wine.data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-29 02:28:55--  https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10782 (11K) [application/x-httpd-php]\n",
            "Saving to: ‘wine.data’\n",
            "\n",
            "\rwine.data             0%[                    ]       0  --.-KB/s               \rwine.data           100%[===================>]  10.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-11-29 02:28:55 (71.7 MB/s) - ‘wine.data’ saved [10782/10782]\n",
            "\n",
            "1,14.23,1.71,2.43,15.6,127,2.8,3.06,.28,2.29,5.64,1.04,3.92,1065\n",
            "1,13.2,1.78,2.14,11.2,100,2.65,2.76,.26,1.28,4.38,1.05,3.4,1050\n",
            "1,13.16,2.36,2.67,18.6,101,2.8,3.24,.3,2.81,5.68,1.03,3.17,1185\n",
            "1,14.37,1.95,2.5,16.8,113,3.85,3.49,.24,2.18,7.8,.86,3.45,1480\n",
            "1,13.24,2.59,2.87,21,118,2.8,2.69,.39,1.82,4.32,1.04,2.93,735\n",
            "1,14.2,1.76,2.45,15.2,112,3.27,3.39,.34,1.97,6.75,1.05,2.85,1450\n",
            "1,14.39,1.87,2.45,14.6,96,2.5,2.52,.3,1.98,5.25,1.02,3.58,1290\n",
            "1,14.06,2.15,2.61,17.6,121,2.6,2.51,.31,1.25,5.05,1.06,3.58,1295\n",
            "1,14.83,1.64,2.17,14,97,2.8,2.98,.29,1.98,5.2,1.08,2.85,1045\n",
            "1,13.86,1.35,2.27,16,98,2.98,3.15,.22,1.85,7.22,1.01,3.55,1045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPl6_up06Hyq",
        "outputId": "6d5bdcd0-f69f-4926-c9d4-7ee8d2abacdc"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "        xy = np.loadtxt('./wine.data', delimiter=',', dtype=np.float32)\n",
        "        self.x = xy[:, 1:]\n",
        "        self.y = xy[:, [0]]\n",
        "        self.nsamples = xy.shape[0]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.x[index], self.y[index]\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nsamples\n",
        "\n",
        "class ToTensor:\n",
        "    def __call__(self, sample):\n",
        "        inputs, labels = sample\n",
        "        return torch.from_numpy(inputs), torch.from_numpy(labels)\n",
        "        \n",
        "\n",
        "# dataset = WineDataset(transform=ToTensor())\n",
        "dataset = WineDataset(transform=None)\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(type(features), type(labels))\n",
        "\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# data_iter = iter(dataloader)\n",
        "# data = data_iter.next()\n",
        "# features, labels = data\n",
        "# print(features, labels)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 2\n",
        "total_samples = len(dataset)\n",
        "n_iterations = math.ceil(total_samples / 4)\n",
        "print(total_samples, n_iterations)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(dataloader):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if (i > 0 and i % 10 == 0) or (i+1==n_iterations):\n",
        "            print(f'epoch={epoch+1}/{num_epochs}, step={i+1}/{n_iterations}, inputs {inputs.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "178 45\n",
            "epoch=1/2, step=11/45, inputs torch.Size([4, 13])\n",
            "epoch=1/2, step=21/45, inputs torch.Size([4, 13])\n",
            "epoch=1/2, step=31/45, inputs torch.Size([4, 13])\n",
            "epoch=1/2, step=41/45, inputs torch.Size([4, 13])\n",
            "epoch=1/2, step=45/45, inputs torch.Size([2, 13])\n",
            "epoch=2/2, step=11/45, inputs torch.Size([4, 13])\n",
            "epoch=2/2, step=21/45, inputs torch.Size([4, 13])\n",
            "epoch=2/2, step=31/45, inputs torch.Size([4, 13])\n",
            "epoch=2/2, step=41/45, inputs torch.Size([4, 13])\n",
            "epoch=2/2, step=45/45, inputs torch.Size([2, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnvJhiAqBgCA",
        "outputId": "c91ebe2b-21c7-47c6-8a9b-74e6b936c013"
      },
      "source": [
        "len(dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaHLD9aGfB78"
      },
      "source": [
        "## transforms for the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvIJ4T8bfBh9",
        "outputId": "bdf1e022-7988-41eb-f044-47ffdcef851f"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "        xy = np.loadtxt('./wine.data', delimiter=',', dtype=np.float32)\n",
        "        self.x = xy[:, 1:]\n",
        "        self.y = xy[:, [0]]\n",
        "        self.nsamples = xy.shape[0]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.x[index], self.y[index]\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nsamples\n",
        "\n",
        "class ToTensor:\n",
        "    def __call__(self, sample):\n",
        "        inputs, labels = sample\n",
        "        return torch.from_numpy(inputs), torch.from_numpy(labels)\n",
        "        \n",
        "\n",
        "# dataset = WineDataset(transform=ToTensor())\n",
        "dataset = WineDataset(transform=None)\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(features)\n",
        "print(type(features), type(labels))\n",
        "\n",
        "class MulTransform:\n",
        "    # multiply inputs with a given factor\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        inputs, targets = sample\n",
        "        inputs *= self.factor\n",
        "        return inputs, targets\n",
        "\n",
        "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(2)])\n",
        "dataset = WineDataset(transform=composed)\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(features)\n",
        "print(type(features), type(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
            " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "tensor([2.8460e+01, 3.4200e+00, 4.8600e+00, 3.1200e+01, 2.5400e+02, 5.6000e+00,\n",
            "        6.1200e+00, 5.6000e-01, 4.5800e+00, 1.1280e+01, 2.0800e+00, 7.8400e+00,\n",
            "        2.1300e+03])\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPxC-caskCM8"
      },
      "source": [
        "## softmax and cross entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwdMjqMPkEHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5054ed5-600b-4420-a629-53cd2bc2f9e5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "x = np.array([2., 1., 0.1])\n",
        "outputs = softmax(x)\n",
        "print('softmax numpy:', outputs)\n",
        "\n",
        "x = torch.tensor([2., 1., 0.1])\n",
        "outputs = torch.softmax(x, dim=0)\n",
        "print(outputs)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax numpy: [0.65900114 0.24243297 0.09856589]\n",
            "tensor([0.6590, 0.2424, 0.0986])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def crossentropy(actual, predicted):\n",
        "    loss = -np.sum(actual * np.log(predicted))\n",
        "    return loss\n",
        "\n",
        "Y = np.array([1, 0, 0])\n",
        "\n",
        "y_pred_good = np.array([0.7, 0.2, 0.1])\n",
        "y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
        "l1 = crossentropy(Y, y_pred_good)\n",
        "l2 = crossentropy(Y, y_pred_bad)\n",
        "\n",
        "print(f'loss l1: {l1}')\n",
        "print(f'loss l2: {l2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5UM0TInILTb",
        "outputId": "a433ceeb-f559-4c41-dd9a-aaa390659141"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss l1: 0.35667494393873245\n",
            "loss l2: 2.3025850929940455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "Y = torch.tensor([0])\n",
        "\n",
        "# nsamples x nclasses = 1x3\n",
        "Y_pred_good = torch.tensor([[2., 1., 0.1]])\n",
        "Y_pred_bad = torch.tensor([[0.5, 2., 0.3]])\n",
        "\n",
        "l1 = loss(Y_pred_good, Y)\n",
        "l2 = loss(Y_pred_bad, Y)\n",
        "\n",
        "\n",
        "print(f'loss l1: {l1.item()}')\n",
        "print(f'loss l2: {l2.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_aKHT6I8B86",
        "outputId": "cf8a5ad7-4e38-429b-81a6-90c81e223125"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss l1: 0.4170299470424652\n",
            "loss l2: 1.840616226196289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## multiclass python"
      ],
      "metadata": {
        "id": "seUJR_eB-yRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NN2(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NN2, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        # no softmax at the end\n",
        "        return out\n",
        "\n",
        "model = NN2(input_size=28*28, hidden_size=5, num_classes=3)\n",
        "criterion = nn.CrossEntropyLoss() # applies softmax\n"
      ],
      "metadata": {
        "id": "xCFYFqPy9sxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## binary class"
      ],
      "metadata": {
        "id": "eoAd2Zpo_7-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NN1(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(NN2, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        # no softmax at the end\n",
        "        return out\n",
        "\n",
        "model = NN2(input_size=28*28, hidden_size=5)\n",
        "criterion = nn.BCELoss() # applies softmax\n"
      ],
      "metadata": {
        "id": "5osEmQJs_9uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## activation functions"
      ],
      "metadata": {
        "id": "BfFpsAqtAUGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KDfgqcrnAVQR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}