{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace challenge - Debugger notebook\n",
    "Run this notebook to verify your libraries versions, check GPU config and run a quick training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2utsYSKszvv"
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "import multiprocessing\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "import soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print main infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5P6I-W9ts-kR",
    "outputId": "939bd550-1486-46a6-8371-e82ada0f448c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Linux-5.11.0-37-generic-x86_64-with-glibc2.10\n",
      "CPU cores: 60\n",
      "Python version: 3.8.8\n",
      "PyTorch version: 1.10.1+cu102\n",
      "GPU is visible: True\n",
      "Transformers version: 4.16.0.dev0\n",
      "Datasets version: 1.17.1.dev0\n",
      "soundfile version: 0.10.3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"CPU cores: {multiprocessing.cpu_count()}\")\n",
    "\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU is visible: {torch.cuda.is_available()}\")\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Datasets version: {datasets.__version__}\")\n",
    "\n",
    "print(f\"soundfile version: {soundfile.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your GPU informations (if any)\n",
    "If you launched an AI Training job with GPU resources, they should be listed below (Tesla V100s 32GB).\n",
    "Driver and CUDA version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YT7fRnKctggU",
    "outputId": "f355a3e0-20da-489f-bd1f-5e508e792a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 22 02:48:58 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100S-PCI...  Off  | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   35C    P0    26W / 250W |      4MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f72bffe678b4bdca366b35305baaab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TorMtpwPv6RQ"
   },
   "source": [
    "## Quick training run with a dummy model and data\n",
    "more information on https://github.com/huggingface/transformers/tree/master/examples/pytorch/speech-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fevoJD15u4Ss",
    "outputId": "5861d34e-745b-45ee-e780-ed363043e655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-22 02:49:05--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30348 (30K) [text/plain]\n",
      "Saving to: ‘run_speech_recognition_ctc.py’\n",
      "\n",
      "run_speech_recognit 100%[===================>]  29.64K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2022-01-22 02:49:05 (21.7 MB/s) - ‘run_speech_recognition_ctc.py’ saved [30348/30348]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O run_speech_recognition_ctc.py https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mz4bubhxxsad",
    "outputId": "23398525-cc19-43c2-9fec-497e06214f29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/22/2022 02:49:17 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "01/22/2022 02:49:17 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.STEPS,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=True,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=7.5e-05,\n",
      "length_column_name=input_length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./Wav2Vec2-Large-XLSR-53-Assamese/runs/Jan22_02-49-17_job-8be8b741-e32e-4579-bbec-1e00d9824b4f,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=400.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=./Wav2Vec2-Large-XLSR-53-Assamese,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./Wav2Vec2-Large-XLSR-53-Assamese,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=500,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "01/22/2022 02:49:20 - WARNING - datasets.builder - Reusing dataset common_voice (/workspace/.cache/huggingface/datasets/mozilla-foundation___common_voice/as/7.0.0/33e08856cfa0d0665e837bcad73ffd920a0bc713ce8c5fffb55dbdf1c084d5ba)\n",
      "01/22/2022 02:49:22 - WARNING - datasets.builder - Reusing dataset common_voice (/workspace/.cache/huggingface/datasets/mozilla-foundation___common_voice/as/7.0.0/33e08856cfa0d0665e837bcad73ffd920a0bc713ce8c5fffb55dbdf1c084d5ba)\n",
      "remove special characters from datasets: 100%|█| 624/624 [00:00<00:00, 5143.14ex\n",
      "remove special characters from datasets: 100%|█| 281/281 [00:00<00:00, 6707.79ex\n",
      "https://huggingface.co/infinitejoy/Wav2Vec2-Large-XLSR-53-Assamese/resolve/main/config.json not found in cache or force_download set to True, downloading to /workspace/.cache/huggingface/transformers/tmpa05ahwit\n",
      "Downloading: 100%|██████████████████████████| 1.52k/1.52k [00:00<00:00, 792kB/s]\n",
      "storing https://huggingface.co/infinitejoy/Wav2Vec2-Large-XLSR-53-Assamese/resolve/main/config.json in cache at /workspace/.cache/huggingface/transformers/d80c6e61b0c406416c441cc6fbf23eb87fe3b62547d990f9a318eded7cede943.0feefbddfacf61d6ec0d42b978f445f64740c91086f81c2ba6b2f08a40dd8450\n",
      "creating metadata file for /workspace/.cache/huggingface/transformers/d80c6e61b0c406416c441cc6fbf23eb87fe3b62547d990f9a318eded7cede943.0feefbddfacf61d6ec0d42b978f445f64740c91086f81c2ba6b2f08a40dd8450\n",
      "loading configuration file https://huggingface.co/infinitejoy/Wav2Vec2-Large-XLSR-53-Assamese/resolve/main/config.json from cache at /workspace/.cache/huggingface/transformers/d80c6e61b0c406416c441cc6fbf23eb87fe3b62547d990f9a318eded7cede943.0feefbddfacf61d6ec0d42b978f445f64740c91086f81c2ba6b2f08a40dd8450\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:349: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"infinitejoy/Wav2Vec2-Large-XLSR-53-Assamese\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 64,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.16.0.dev0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 65,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 37.69ba/s]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 107.13ba/s]\n",
      "Didn't find file ./Wav2Vec2-Large-XLSR-53-Assamese/tokenizer_config.json. We won't load it.\n",
      "Didn't find file ./Wav2Vec2-Large-XLSR-53-Assamese/added_tokens.json. We won't load it.\n",
      "Didn't find file ./Wav2Vec2-Large-XLSR-53-Assamese/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./Wav2Vec2-Large-XLSR-53-Assamese/tokenizer.json. We won't load it.\n",
      "loading file ./Wav2Vec2-Large-XLSR-53-Assamese/vocab.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "file ./Wav2Vec2-Large-XLSR-53-Assamese/config.json not found\n",
      "Adding <s> to the vocabulary\n",
      "Adding </s> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file https://huggingface.co/infinitejoy/Wav2Vec2-Large-XLSR-53-Assamese/resolve/main/config.json from cache at /workspace/.cache/huggingface/transformers/d80c6e61b0c406416c441cc6fbf23eb87fe3b62547d990f9a318eded7cede943.0feefbddfacf61d6ec0d42b978f445f64740c91086f81c2ba6b2f08a40dd8450\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"infinitejoy/Wav2Vec2-Large-XLSR-53-Assamese\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 64,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.16.0.dev0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 65,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "https://huggingface.co/infinitejoy/Wav2Vec2-Large-XLSR-53-Assamese/resolve/main/preprocessor_config.json not found in cache or force_download set to True, downloading to /workspace/.cache/huggingface/transformers/tmpiryhzjw6\n",
      "Downloading: 100%|█████████████████████████████| 158/158 [00:00<00:00, 84.3kB/s]\n",
      "storing https://huggingface.co/infinitejoy/Wav2Vec2-Large-XLSR-53-Assamese/resolve/main/preprocessor_config.json in cache at /workspace/.cache/huggingface/transformers/96827e5aeead13bf8e79b576fe8a14f03aed592d72b88188526a0c0e61e0c347.fcd266b775b7f33ba9b607a0fee7cc615aeb2eb281586f046280492ea380ae23\n",
      "creating metadata file for /workspace/.cache/huggingface/transformers/96827e5aeead13bf8e79b576fe8a14f03aed592d72b88188526a0c0e61e0c347.fcd266b775b7f33ba9b607a0fee7cc615aeb2eb281586f046280492ea380ae23\n",
      "loading feature extractor configuration file https://huggingface.co/infinitejoy/Wav2Vec2-Large-XLSR-53-Assamese/resolve/main/preprocessor_config.json from cache at /workspace/.cache/huggingface/transformers/96827e5aeead13bf8e79b576fe8a14f03aed592d72b88188526a0c0e61e0c347.fcd266b775b7f33ba9b607a0fee7cc615aeb2eb281586f046280492ea380ae23\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "https://huggingface.co/infinitejoy/Wav2Vec2-Large-XLSR-53-Assamese/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /workspace/.cache/huggingface/transformers/tmp3144nhdx\n",
      "Downloading: 100%|█████████████████████████| 1.18G/1.18G [00:40<00:00, 31.3MB/s]\n",
      "storing https://huggingface.co/infinitejoy/Wav2Vec2-Large-XLSR-53-Assamese/resolve/main/pytorch_model.bin in cache at /workspace/.cache/huggingface/transformers/654562ea6c5af80d5936ea5e155776f1b8d872cae3e13985adf6262ac6e0ffc8.d16166fd7b568d5baa286f1ce1272faa1ee876877a4244e8698c1890b72d9e80\n",
      "creating metadata file for /workspace/.cache/huggingface/transformers/654562ea6c5af80d5936ea5e155776f1b8d872cae3e13985adf6262ac6e0ffc8.d16166fd7b568d5baa286f1ce1272faa1ee876877a4244e8698c1890b72d9e80\n",
      "loading weights file https://huggingface.co/infinitejoy/Wav2Vec2-Large-XLSR-53-Assamese/resolve/main/pytorch_model.bin from cache at /workspace/.cache/huggingface/transformers/654562ea6c5af80d5936ea5e155776f1b8d872cae3e13985adf6262ac6e0ffc8.d16166fd7b568d5baa286f1ce1272faa1ee876877a4244e8698c1890b72d9e80\n",
      "Traceback (most recent call last):\n",
      "  File \"run_speech_recognition_ctc.py\", line 737, in <module>\n",
      "    main()\n",
      "  File \"run_speech_recognition_ctc.py\", line 544, in main\n",
      "    model = AutoModelForCTC.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 447, in from_pretrained\n",
      "    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 1455, in from_pretrained\n",
      "    model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_state_dict_into_model(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 1609, in _load_state_dict_into_model\n",
      "    raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n",
      "RuntimeError: Error(s) in loading state_dict for Wav2Vec2ForCTC:\n",
      "\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([65, 1024]) from checkpoint, the shape in current model is torch.Size([68, 1024]).\n",
      "\tsize mismatch for lm_head.bias: copying a param with shape torch.Size([65]) from checkpoint, the shape in current model is torch.Size([68]).\n"
     ]
    }
   ],
   "source": [
    "!python run_speech_recognition_ctc.py \\\n",
    "\t--dataset_name=\"mozilla-foundation/common_voice_7_0\" \\\n",
    "\t--model_name_or_path=\"infinitejoy/Wav2Vec2-Large-XLSR-53-Assamese\" \\\n",
    "\t--dataset_config_name=\"as\" \\\n",
    "\t--output_dir=\"./Wav2Vec2-Large-XLSR-53-Assamese\" \\\n",
    "\t--overwrite_output_dir \\\n",
    "\t--num_train_epochs=\"400\" \\\n",
    "\t--per_device_train_batch_size=\"16\" \\\n",
    "\t--per_device_eval_batch_size=\"16\" \\\n",
    "\t--gradient_accumulation_steps=\"2\" \\\n",
    "\t--learning_rate=\"7.5e-5\" \\\n",
    "\t--warmup_steps=\"500\" \\\n",
    "\t--length_column_name=\"input_length\" \\\n",
    "\t--evaluation_strategy=\"steps\" \\\n",
    "\t--text_column_name=\"sentence\" \\\n",
    "\t--chars_to_ignore , ? . ! \\- \\; \\: \\\" “ % ‘ ” � \\। \\\n",
    "\t--save_steps=\"500\" \\\n",
    "\t--eval_steps=\"500\" \\\n",
    "\t--logging_steps=\"100\" \\\n",
    "\t--layerdrop=\"0.0\" \\\n",
    "\t--activation_dropout=\"0.1\" \\\n",
    "\t--save_total_limit=\"2\" \\\n",
    "\t--freeze_feature_encoder \\\n",
    "\t--feat_proj_dropout=\"0.0\" \\\n",
    "\t--mask_time_prob=\"0.75\" \\\n",
    "\t--mask_time_length=\"10\" \\\n",
    "\t--mask_feature_prob=\"0.25\" \\\n",
    "\t--mask_feature_length=\"64\" \\\n",
    "\t--gradient_checkpointing \\\n",
    "\t--use_auth_token \\\n",
    "\t--fp16 \\\n",
    "\t--group_by_length \\\n",
    "\t--do_train --do_eval \\\n",
    "    --push_to_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r wav2vec2-large-xls-r-300m-odia.zip wav2vec2-large-xls-r-300m-odia/\n",
    "# !rm wav2vec2-large-xls-r-300m-odia.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "overlay         3.5T  557G  2.8T  17% /\n",
      "tmpfs            64M     0   64M   0% /dev\n",
      "tmpfs            87G     0   87G   0% /sys/fs/cgroup\n",
      "tmpfs            87G     0   87G   0% /dev/shm\n",
      "/dev/md0        3.5T  557G  2.8T  17% /etc/group\n",
      "tmpfs            87G   12K   87G   1% /proc/driver/nvidia\n",
      "/dev/vda1        49G  6.6G   42G  14% /usr/bin/nvidia-smi\n",
      "udev             87G     0   87G   0% /dev/nvidia0\n",
      "tmpfs            87G     0   87G   0% /proc/acpi\n",
      "tmpfs            87G     0   87G   0% /proc/scsi\n",
      "tmpfs            87G     0   87G   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset common_voice (/workspace/.cache/huggingface/datasets/mozilla-foundation___common_voice/or/7.0.0/33e08856cfa0d0665e837bcad73ffd920a0bc713ce8c5fffb55dbdf1c084d5ba)\n",
      "Reusing dataset common_voice (/workspace/.cache/huggingface/datasets/mozilla-foundation___common_voice/or/7.0.0/33e08856cfa0d0665e837bcad73ffd920a0bc713ce8c5fffb55dbdf1c084d5ba)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, Audio\n",
    "\n",
    "common_voice_train = load_dataset(\"mozilla-foundation/common_voice_7_0\", \"or\", use_auth_token=True, split=\"train+validation\")\n",
    "common_voice_test = load_dataset(\"mozilla-foundation/common_voice_7_0\", \"or\", use_auth_token=True, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2013.75"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_voice_train) * 120 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "common_voice_test = common_voice_test.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ସେ କଥା ଯାଉ, ଆମ୍ଭମାନଙ୍କୁ ଆଉ କଥା ଲେଖିବାକୁ ହେବ ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ଯାହା ଦରମା ଗଣ୍ଡାକ ପାଉଥିଲେ, ପେଟ ପିଠିକୁ ନିଅଣ୍ଟ, ବିଧବା ଲାଗି ସାଇତି ଯିବେ କଣ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ଯେ ଯେଡ଼େ ହୁସିଆର ହେବ, ଆପଦ ବିପଦ କାହାରିକୁ ଛାଡ଼ିନାହିଁ ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ମୁଁ ପୂଜା ସାରି ସେମାନଙ୍କୁ କିଛି ଭୋଗ ଦେଇ ଘରେ ଛାଡ଼ିଆସିଲି, ବାକି ଭୋଗକୁ ବାନ୍ଧିଲି ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ବାସୁ ଦୁଇ ଟଙ୍କାର ନଡ଼ା କିଣି ବାଡ଼ିରେ ଗଦେଇଅଛି, ଶରଣ ଦେବାରୁ ଛପରବନ୍ଦି ହୋଇପାରି ନାହିଁ ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ଦେଖି ଦେଖି ମନରେ କଲା, ଆଜି ଏ କଣ ହେଉଛି ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ଶାଶୁ ମୁହଁକୁ ଚାହିଁ ଗାଳି ଦିଏ ନାହିଁ; ଓଢ଼ଣା ପଡ଼ିଥାଏ, ପଛ କରି ବରବର କରି ବକିଯାଏ ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ଆଜି ମହାପ୍ରସାଦ ଉଠା ପରା ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"\"\"ଯାହାର ବାହା ସେ ଖେଳୁଛି ପଶା ଧାଇଁ ବୁଲୁଛନ୍ତି ସାଇ ପଡିଶା ।\"\"\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ଅଶୀ ବର୍ଷର ପୁରୁଷ ବି ବିଭା ହୋଇ ପାରେ ।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(common_voice_train.remove_columns([\"path\", \"audio\"]), num_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_remove_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\'\\’\\–]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_remove_regex, '', batch[\"sentence\"]).lower()\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9df324b393840628b6a038aa00aa697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/537 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fb5a05a9484b5892a1b41be076c55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.map(remove_special_characters)\n",
    "common_voice_test = common_voice_test.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_hatted_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub('[â]', 'a', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[î]', 'i', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[ô]', 'o', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[û]', 'u', batch[\"sentence\"])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57ed45f40c440dc8df26f140b226c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/537 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863c62f8741f4efcb130ff1f44f3e0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.map(replace_hatted_characters)\n",
    "common_voice_test = common_voice_test.map(replace_hatted_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"sentence\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ab1aeb1bb240ca821b5558280495f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d8bf90831a4e5d8a3feb7f30cf5966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names)\n",
    "vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '|': 1,\n",
       " '।': 2,\n",
       " 'ଁ': 3,\n",
       " 'ଂ': 4,\n",
       " 'ଃ': 5,\n",
       " 'ଅ': 6,\n",
       " 'ଆ': 7,\n",
       " 'ଇ': 8,\n",
       " 'ଈ': 9,\n",
       " 'ଉ': 10,\n",
       " 'ଊ': 11,\n",
       " 'ଏ': 12,\n",
       " 'ଓ': 13,\n",
       " 'କ': 14,\n",
       " 'ଖ': 15,\n",
       " 'ଗ': 16,\n",
       " 'ଘ': 17,\n",
       " 'ଙ': 18,\n",
       " 'ଚ': 19,\n",
       " 'ଛ': 20,\n",
       " 'ଜ': 21,\n",
       " 'ଝ': 22,\n",
       " 'ଞ': 23,\n",
       " 'ଟ': 24,\n",
       " 'ଠ': 25,\n",
       " 'ଡ': 26,\n",
       " 'ଢ': 27,\n",
       " 'ଣ': 28,\n",
       " 'ତ': 29,\n",
       " 'ଥ': 30,\n",
       " 'ଦ': 31,\n",
       " 'ଧ': 32,\n",
       " 'ନ': 33,\n",
       " 'ପ': 34,\n",
       " 'ଫ': 35,\n",
       " 'ବ': 36,\n",
       " 'ଭ': 37,\n",
       " 'ମ': 38,\n",
       " 'ଯ': 39,\n",
       " 'ର': 40,\n",
       " 'ଲ': 41,\n",
       " 'ଳ': 42,\n",
       " 'ଵ': 43,\n",
       " 'ଶ': 44,\n",
       " 'ଷ': 45,\n",
       " 'ସ': 46,\n",
       " 'ହ': 47,\n",
       " '଼': 48,\n",
       " 'ା': 49,\n",
       " 'ି': 50,\n",
       " 'ୀ': 51,\n",
       " 'ୁ': 52,\n",
       " 'ୂ': 53,\n",
       " 'ୃ': 54,\n",
       " 'େ': 55,\n",
       " 'ୈ': 56,\n",
       " 'ୋ': 57,\n",
       " 'ୌ': 58,\n",
       " '୍': 59,\n",
       " 'ୟ': 60,\n",
       " 'ୱ': 61}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-21 08:33:50--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/robust-speech-event/eval.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4419 (4.3K) [text/plain]\n",
      "Saving to: ‘eval.py’\n",
      "\n",
      "eval.py             100%[===================>]   4.32K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-01-21 08:33:50 (14.9 MB/s) - ‘eval.py’ saved [4419/4419]\n",
      "\n",
      "total 1232676\n",
      "-rw-r--r-- 1 ovh ovh        686 Jan 21 06:29 vocab.json\n",
      "-rw-r--r-- 1 ovh ovh        290 Jan 21 06:29 tokenizer_config.json\n",
      "-rw-r--r-- 1 ovh ovh        502 Jan 21 06:29 special_tokens_map.json\n",
      "-rw-r--r-- 1 ovh ovh         23 Jan 21 06:29 added_tokens.json\n",
      "drwxr-xr-x 2 ovh ovh       4096 Jan 21 07:02 checkpoint-1000\n",
      "drwxr-xr-x 2 ovh ovh       4096 Jan 21 07:19 checkpoint-1500\n",
      "drwxr-xr-x 2 ovh ovh       4096 Jan 21 07:37 checkpoint-2000\n",
      "-rw-r--r-- 1 ovh ovh       3953 Jan 21 07:39 trainer_state.json\n",
      "-rw-r--r-- 1 ovh ovh        194 Jan 21 07:39 train_results.json\n",
      "-rw-r--r-- 1 ovh ovh        222 Jan 21 07:39 eval_results.json\n",
      "-rw-r--r-- 1 ovh ovh       2033 Jan 21 07:39 config.json\n",
      "-rw-r--r-- 1 ovh ovh        394 Jan 21 07:39 all_results.json\n",
      "-rw-r--r-- 1 ovh ovh 1262186097 Jan 21 07:39 pytorch_model.bin\n",
      "-rw-r--r-- 1 ovh ovh       3055 Jan 21 07:39 training_args.bin\n",
      "-rw-r--r-- 1 ovh ovh        212 Jan 21 07:39 preprocessor_config.json\n",
      "-rw-r--r-- 1 ovh ovh       1825 Jan 21 07:41 README.md\n",
      "-rw-r--r-- 1 ovh ovh       4419 Jan 21 08:33 eval.py\n"
     ]
    }
   ],
   "source": [
    "!wget -O eval.py https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/robust-speech-event/eval.py\n",
    "!cp eval.py wav2vec2-large-xls-r-300m-odia\n",
    "!ls -ltr wav2vec2-large-xls-r-300m-odia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing dataset common_voice (/workspace/.cache/huggingface/datasets/mozilla-foundation___common_voice/or/7.0.0/33e08856cfa0d0665e837bcad73ffd920a0bc713ce8c5fffb55dbdf1c084d5ba)\n",
      "100%|███████████████████████████████████████████| 10/10 [00:06<00:00,  1.55ex/s]\n",
      "Downloading: 5.61kB [00:00, 2.23MB/s]                                           \n",
      "WER: 1.0921052631578947\n",
      "CER: 2.5547945205479454\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 13001.56ex/s]\n"
     ]
    }
   ],
   "source": [
    "!cd wav2vec2-large-xls-r-300m-odia; python eval.py --model_id ./ --dataset mozilla-foundation/common_voice_7_0 --config or --split test --log_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM3OaMlm9YQtKpl28c8gBBd",
   "include_colab_link": true,
   "name": "DebugOVHTransformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
